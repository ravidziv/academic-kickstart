---
# Display name
title: Ravid Shwartz-Ziv

# Username (this should match the folder name)
authors:
- admin

# Is this the primary user of the site?
superuser: true

# Role/position
role: Senior Research scientist, Assistant Professor and Faculty Fellow,

# Organizations/Affiliations
organizations:
- name: Center for Data Science, New York University
  url: ""
- name: Wand AI
  url: ""

# Short bio (displayed in user profile at end of posts)
interests:
- Large Language Models (LLMs)
- Model Efficiency & Compression
- Information Theory
- Neural Network Interpretability
- Self-Supervised Learning
- Representation Learning
- Multi-Agent Systems
- Personalization in AI

education:
  courses:
  - course: Ph.D. in Computational Neuroscience
    institution: Hebrew University of Jerusalem
    year: 2021
    details: "Dissertation: Information Flow in Deep Neural Networks"
  - course: B.Sc. in Computer Science and Computational Biology
    institution: Hebrew University of Jerusalem
    year: 2014
    details: "GPA: 91.4/100"

# Social/Academic Networking
social:
- icon: envelope
  icon_pack: fas
  link: 'mailto:ravidziv@gmail.com'
- icon: google-scholar
  icon_pack: ai
  link: https://scholar.google.com/citations?user=SqsLFwMAAAAJ
- icon: github
  icon_pack: fab
  link: https://github.com/ravidziv

# Email for Gravatar
email: "ravidziv@gmail.com"

# Highlight the author in author lists? (true/false)
highlight_name: true

# Organizational groups
user_groups:
- Faculty
- Researchers
---

## Biography

I am an Assistant Professor and Faculty Fellow at NYU's Center for Data Science, where I lead cutting-edge research in artificial intelligence, with a particular focus on Large Language Models (LLMs) and their applications. My work spans theoretical foundations and practical implementations, combining academic rigor with industry impact.

## Research Focus

My research bridges fundamental theoretical understanding with practical applications in AI, particularly focusing on:

* Pioneering novel approaches for analyzing LLM representations and intermediate layer dynamics
* Developing efficient model adaptation and personalization techniques
* Advancing information-theoretic frameworks for understanding neural networks
* Creating innovative benchmarking frameworks for evaluation of AI systems

## Notable Achievements

* Pioneer in applying information theory to neural networks, with seminal work featured in Quanta Magazine and Wired
* Led development of LiveBench, a challenging contamination-free LLM benchmark
* Achieved 60% reduction in computational costs through novel LLM personalization techniques
* Published extensively in top-tier venues (NeurIPS, ICLR, ICML)
* Google PhD Fellowship recipient (2018-2021)
* Best Paper Award, Information Fusion journal (2023)
* CPAL Rising Star Award, The University of Hong Kong (2023)
* Moore-Sloan Fellowship, NYU (2021-2022)

## Current Projects

* Leading research initiatives in LLM personalization and adaptation
* Developing novel approaches for analyzing and improving model efficiency
* Creating new frameworks for understanding information flow in large-scale models
* Advancing multi-agent systems and user-centric search implementations

My work combines theoretical insights with practical applications, contributing to both the academic understanding of AI systems and their real-world implementation. Through my dual role in academia and industry, I strive to bridge the gap between theoretical breakthroughs and practical applications in AI.
